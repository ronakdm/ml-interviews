{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathbf{x} = (X_1, ..., x_T)\\in \\mathbb{R}^{d \\times T}$ be a sequence. Let $h \\in \\{1, ..., H\\}$, where $H$ is the *number of heads*. Define\n",
    "\\begin{align*}\n",
    "    Q^{(h)}(x_i) &= W^{(h)}_Q x_i\\\\ \n",
    "    K^{(h)}(x_i) &= W^{(h)}_K x_i\\\\ \n",
    "    V^{(h)}(x_i) &= W^{(h)}_V x_i\\\\ \n",
    "\\end{align*}\n",
    "for $W^{(h)}_Q, W^{(h)}_K, W^{(h)}_V \\in \\mathbb{R}^{k \\times d}$ to be a *query*, *key*, and *value*. Define\n",
    "\\begin{align*}\n",
    "    \\alpha^{(h)}_{i, j} = \\text{Softmax}_j\\left(\\frac{Q^{(h)}(x_i)^\\top K^{(h)}(x_i)}{\\sqrt{k}}\\right) \\in (0, 1)\n",
    "\\end{align*}\n",
    "to be the *attention weight*. Define\n",
    "\\begin{align*}\n",
    "    C(x_i) = \\sum_{h=1}^H W^{(h)}_C \\sum_{j=1}^T \\alpha_{i, j} V(x_i)\n",
    "\\end{align*}\n",
    "for $W^{(h)}_C(x_i) \\in \\mathbb{R}^{d \\times k}$ be the *contextualized embedding* of $x_i$. Let\n",
    "\\begin{align*}\n",
    "    u_i &= \\text{LayerNorm}(x_i + C(x_i); \\gamma_1, \\beta_1)\\\\\n",
    "    v_i &= W_2 \\text{ReLU}(W_1 u_i)\\\\\n",
    "    z_i &= \\text{LayerNorm}(u_i + v_i; \\gamma_2, \\beta_2),\n",
    "\\end{align*}\n",
    "where $\\gamma_1, \\beta_1, \\gamma_2, \\beta_2 \\in \\mathbb{R}, W_1 \\in \\mathbb{R}^{m \\times d}, W_2 \\in \\mathbb{R}^{d \\times m}$. The $\\text{LayerNorm}(\\cdot)$ function for $z \\in \\mathbb{R}^p$ is defined by\n",
    "\\begin{align*}\n",
    "    \\text{LayerNorm}(z; \\gamma, \\beta) =  \\gamma \\left(\\frac{z - \\mu_z}{\\sigma_z}\\right) + \\beta,\n",
    "\\end{align*}\n",
    "and $\\mu_z = \\frac{1}{p} \\sum_{k=1}^p z_k$ and $\\sigma^2_z = \\frac{1}{p} \\sum_{k=1}^p (z_k - \\mu_z)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a summary that might lend itself more nicely to an interview conversation:\n",
    "- There are a number of heads, and for each head, there is a linear mapping to a query, key and value.\n",
    "- For each token $i$, there is an attention weight for each token $j$ (including itself) that describes how much $x_i$ should \"attend\" to $x_j$, given by the softmax of the dot product divided by the square root of the dimension.\n",
    "- The values are combined via convex combination using the attention weights, a linear mapping is applied on each head, for which the output is summed together across heads.\n",
    "- There is a LayerNorm layer that consums a residual connection from the input.\n",
    "- Then, there is a single-hidden layer fully-connected network.\n",
    "- Finally, another LayerNorm layer that consumes a residual connection from the output of the previous LayerNorm layer (remember this as LN to LN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
